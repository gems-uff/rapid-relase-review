---
@Article{Vig2018,
  author    = {Vig, Vidhi and Kaur, Arvinder},
  journal   = {Journal of Intelligent \& Fuzzy Systems},
  title     = {Test Effort Estimation and Prediction of Traditional and Rapid Release Models Using Machine Learning Algorithms},
  year      = {2018},
  issn      = {1064-1246},
  month     = jan,
  number    = {2},
  pages     = {1657--1669},
  volume    = {35},
  abstract  = {Recently, many software companies have shifted to shorter release cycles from the traditional multi-month release cycle. Evolution and transition of release cycles may affect the test effort in the system. This paper analyses 25 traditional releases},
  doi       = {10.3233/JIFS-169703},
  groups    = {forward_01, p_Khomh2015_forward, forward_1, khomh2015understanding, mantyla2015rapid, selected, Other Studies},
  language  = {en},
  publisher = {{IOS Press}},
  timestamp = {2020-10-29T21:04:40Z},
}
---

# RQ1. What characterizes a rapid release cycle?

The authors consider 2 months as a rapid release cycle, i.e., projects that release more than six releases per year.

# RQ2. What are the implications of adopting a rapid release cycle?

 1. The authors observed that the OO metrics highly correlated with test metrics remain the same on rapid and traditional release cycles.

 2. The test effort was slightly more in the rapid release cycle, indicating that shorter release cycles might require more test effort in the system.

# RQ3. What metrics do the studies use to compare rapid and traditional releases?

 - Size (LOC, STAT, SLOC, CLOC, Command, Jloc, Jf, Jm, Cons, Com rat, Tcom rat)
 - Complexity (Osmax, Osavg, WMC, NOAC, Ocmax, Ocavg, Opavg, CSO, CSOA, CSA, Query, NAAC, NOIC, NOOC)
 - Coupling (RFC, MPC, CBO)
 - Inheritance (DIT, Dcy, Dcy∗, Dpt, Dpt∗, Cyclic, Level, Level∗, Pdcy, PDpt, Sub, Inner)
 - Halstead (N,E,V,D,B,n)
 - Cohesion (LCOM)

# RQ4. How do the studies mine release information?

The authors mined the projects' GitHub and Jira issue tracker. They used IntellijIdea to extract object-oriented metrics. 

# RQ5. How do the studies evaluate their findings?

The authors conducted statistical tests  (Shapiro Wilk test, Friedman’s Test, and Nemenyi Test) to compare the metrics in traditional and rapid release cycles. They also calculated the correlation (Spearman’s Test) of some of the metrics.

# RQ6. What corpus do the studies use to compare rapid and traditional releases?

 - 25 traditional releases containing 1210 classes and 69 rapid releases containing 2616 classes of four Open Source Java systems from Apache Software Foundation (Avro, Hive, Jclouds, and Zookeeper).

